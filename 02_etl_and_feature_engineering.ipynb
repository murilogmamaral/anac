{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badb35ec",
   "metadata": {},
   "source": [
    "Murilo Amaral  \n",
    "Email: murilogmamaral@gmail.com  \n",
    "Linkedin: https://www.linkedin.com/in/murilogmamaral/  \n",
    "\n",
    "___\n",
    "# Forecasting passenger flow at Brazilian airports\n",
    "## 2 - ETL and feature engineering\n",
    "This notebook aims to extract, transform and enrich data to make it available in refined zone.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5af65ac",
   "metadata": {},
   "source": [
    "### Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275d66c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:13:15.768486Z",
     "start_time": "2022-05-22T03:13:15.754063Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "class color: RED = '\\033[91m'; GREEN = '\\033[92m'; GRAY = '\\033[90m'; END = '\\033[0m'\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b71b75",
   "metadata": {},
   "source": [
    "### From download to raw zone (partitioned by arrival airport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f08dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:13:16.930324Z",
     "start_time": "2022-05-22T03:13:16.924133Z"
    }
   },
   "outputs": [],
   "source": [
    "download_path = '/Volumes/datalake/download/anac/'\n",
    "raw_path      = '/Volumes/datalake/raw/anac/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04927dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:13:17.744466Z",
     "start_time": "2022-05-22T03:13:17.736037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selected columns\n",
    "usecols = ['sg_empresa_icao',       # company\n",
    "           'nm_pais_origem',        # leaving from (country)\n",
    "           'nm_municipio_origem',   # leaving from (city)\n",
    "           'nm_municipio_destino',  # going to (city)\n",
    "           'sg_icao_origem',        # airport of departure\n",
    "           'sg_icao_destino',       # airport of arrival\n",
    "           'dt_referencia',         # departure date (scheduled)  \n",
    "           'dt_chegada_real',       # arrival date (real)\n",
    "           'hr_partida_real',       # departure time (real)\n",
    "           'hr_chegada_real',       # arrival time (real)\n",
    "           'nr_passag_pagos',       # number of passengers (paid tickets)\n",
    "           'nr_passag_gratis'       # number of passengers (free tickets)\n",
    "          ]\n",
    "\n",
    "# Temporary columns to filter internal and regular flights\n",
    "tempcols = ['ds_grupo_di',\n",
    "            'nr_assentos_ofertados'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5eb055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:13:35.308790Z",
     "start_time": "2022-05-22T03:13:35.296433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to make data available in raw zone\n",
    "def toRaw(file):\n",
    "    try:\n",
    "        test = pd.read_csv(raw_path+'history.csv')\n",
    "    except:\n",
    "        test = pd.DataFrame({'data':[]})\n",
    "    try:\n",
    "        if file not in test['data'].tolist():\n",
    "            \n",
    "            # Loading data\n",
    "            df = pd.read_csv(download_path+file,\n",
    "                             sep = ';',\n",
    "                             encoding = 'latin-1',\n",
    "                             low_memory = False,\n",
    "                             usecols = usecols+tempcols)\n",
    "            # Filtering regular flights\n",
    "            df = df[(df['ds_grupo_di'] == 'REGULAR') & (df['nr_assentos_ofertados'] > 0)]\n",
    "            df = df[usecols]\n",
    "            # Types\n",
    "            for col in ['nr_passag_pagos','nr_passag_gratis']:\n",
    "                df[col] = df[col].fillna(0).apply(int)\n",
    "            # Moving data to raw zone (partitioned by arrival airport) # Pandas will append data by default\n",
    "            df.to_parquet(raw_path, index = False, partition_cols='sg_icao_destino')\n",
    "            test = pd.concat([test,pd.DataFrame({'data':[file]})])\n",
    "            test.to_csv(raw_path+'history.csv',index = False)\n",
    "            print(f'{color.GREEN}>>>> Data from \\'{file}\\' were moved to raw zone.{color.END}')\n",
    "        else:\n",
    "            print(f'{color.GRAY}>>>> Data from \\'basica{year}-{month}.zip\\' was already in download zone.{color.END}')\n",
    "    except:\n",
    "        print(f'{color.RED}---- Data from \\'{file}\\' could not be moved to raw zone.{color.END}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e700903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:17:44.862582Z",
     "start_time": "2022-05-22T03:13:37.710894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m>>>> Data from 'basica2014-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2014-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2015-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2016-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2017-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2018-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2019-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2020-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-04.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-05.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-06.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-07.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-08.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-09.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-10.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-11.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2021-12.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2022-01.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2022-02.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2022-03.zip' were moved to raw zone.\u001b[0m\n",
      "\u001b[92m>>>> Data from 'basica2022-04.zip' were moved to raw zone.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "files = [x.split('/')[-1] for x in glob.glob(download_path+'*')]\n",
    "files.sort()\n",
    "for file in files:\n",
    "    toRaw(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ea0f3",
   "metadata": {},
   "source": [
    "\n",
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afdfb648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:18:02.695559Z",
     "start_time": "2022-05-22T03:17:56.276771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dollar-Real currency (monthly)\n",
    "# Data source: IPEA - The Institute for Applied Economic Research (Brazil)\n",
    "url =  'http://ipeadata.gov.br/exibeserie.aspx?serid=38389'\n",
    "dollar = pd.read_html(url,header=0,decimal=',', thousands='.')[2]\n",
    "dollar.columns = ['year','dollar']\n",
    "dollar['month'] = dollar['year'].apply(lambda x: int(str(x)[4:6]))\n",
    "dollar['year'] = dollar['year'].apply(lambda x: int(str(x)[0:4]))\n",
    "dollar = dollar[dollar.year>=2012]\n",
    "dollar['dollar'] = dollar['dollar'].apply(lambda x: float(x))\n",
    "dollar = dollar[['year','month','dollar']]\n",
    "dollar_spark = spark.createDataFrame(dollar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1d1239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:18:03.306182Z",
     "start_time": "2022-05-22T03:18:02.698608Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unemployment rate in Brazil (monthly)\n",
    "# Data source: IPEA - The Institute for Applied Economic Research (Brazil)\n",
    "url = 'http://www.ipeadata.gov.br/ExibeSerie.aspx?serid=1347352645'\n",
    "unemployment_rate = pd.read_html(url,header=0,decimal=',', thousands='.')[2]\n",
    "unemployment_rate.columns = ['year','unemployment_rate']\n",
    "unemployment_rate['month'] = unemployment_rate['year'].apply(lambda x: int(str(x)[4:6]))\n",
    "unemployment_rate['year'] = unemployment_rate['year'].apply(lambda x: int(str(x)[0:4]))\n",
    "unemployment_rate['unemployment_rate'] = unemployment_rate['unemployment_rate'].apply(lambda x: float(x))\n",
    "unemployment_rate = unemployment_rate[['year','month','unemployment_rate']]\n",
    "unemployment_rate = unemployment_rate[unemployment_rate['year']>=2012].round()\n",
    "unemployment_rate_spark = spark.createDataFrame(unemployment_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8675b99",
   "metadata": {},
   "source": [
    "### From raw to refined zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc85a1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:18:06.125832Z",
     "start_time": "2022-05-22T03:18:06.119599Z"
    }
   },
   "outputs": [],
   "source": [
    "refined_path  = '/Volumes/datalake/refined/anac/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d231250d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:18:09.794882Z",
     "start_time": "2022-05-22T03:18:09.779560Z"
    }
   },
   "outputs": [],
   "source": [
    "def toRefined(departure_airport):\n",
    "\n",
    "    try:\n",
    "        ### EXTRACT ###########################\n",
    "        extract_path = f'/Volumes/datalake/raw/anac/sg_icao_destino={departure_airport}'\n",
    "        df = spark.read.parquet(extract_path).distinct()\n",
    "        \n",
    "        ### TRANSFORM ###########################\n",
    "        # Setting new column names\n",
    "        columns = ['company','departure_country','departure_city','arrival_city',\n",
    "                   'departure_airport','departure_date','arrival_date',\n",
    "                   'departure_hour','arrival_hour','pax','free_pax']\n",
    "        for a, b in zip(df.columns, columns):\n",
    "            df = df.withColumnRenamed(a, b)\n",
    "\n",
    "        # Unifying number of passengers\n",
    "        df = df.withColumn('pax', df['pax'] + df['free_pax'])\n",
    "\n",
    "        # Dropping NAs\n",
    "        df = df.where(df['arrival_date'].isNotNull())\n",
    "\n",
    "        # Getting year, month, day and weekday separately\n",
    "        df = df.withColumn('year',F.year(F.col('arrival_date')))\n",
    "        df = df.withColumn('month',F.month(F.col('arrival_date')))\n",
    "        df = df.withColumn('day',F.dayofmonth(F.col('arrival_date')))\n",
    "        df = df.withColumn('weekday',F.dayofweek(F.col('arrival_date')))\n",
    "\n",
    "        # Join\n",
    "        df = df.join(dollar_spark, on=['year','month'], how = 'left')\n",
    "        df = df.join(unemployment_rate_spark, on=['year','month'], how = 'left')\n",
    "\n",
    "        # Selecting columns\n",
    "        df = df.select('company','departure_country','departure_city','departure_airport',\n",
    "                       'arrival_date','arrival_hour','year','month','day','weekday',\n",
    "                       'pax','dollar','unemployment_rate')\n",
    "        \n",
    "        ### LOAD ###########################\n",
    "        load_path = '/Volumes/datalake/refined/anac/'\n",
    "        (\n",
    "            df\n",
    "            .coalesce(1)\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .mode('overwrite')\n",
    "            .option(\"header\", \"true\")\n",
    "            .save(load_path+f'{departure_airport}')\n",
    "        )\n",
    "\n",
    "        print(f'[{departure_airport}] ETL completed.')\n",
    "        \n",
    "    except:\n",
    "        print(f'[{departure_airport}] Error.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cd39220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T03:20:49.883130Z",
     "start_time": "2022-05-22T03:20:49.874922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SBSV] ETL completed.\n"
     ]
    }
   ],
   "source": [
    "# Getting arrival data of Salvador/BA airport\n",
    "airports = ['SBSV']\n",
    "for a in airports:\n",
    "    toRefined(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
